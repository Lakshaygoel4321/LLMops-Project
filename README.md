# ğŸ§  Medical Research Chatbot using LLM and Pinecone

This is an **LLMOps-powered Medical Research Assistant** that allows users to query a local PDF (e.g., clinical trial, medical research) and get accurate, context-aware responses via a chatbot interface. It utilizes **LangChain**, **Pinecone vector database**, and **Flask** for the backend, and offers a chat UI with **chat history and timestamps**.

---

## ğŸš€ Features

* ğŸ’¬ **Chatbot Interface** with timestamps and history
* ğŸ“„ **PDF-based knowledge ingestion** (Medical Reports / Trials)
* ğŸ§  **LLM-powered context retrieval** using LangChain
* ğŸ“¦ **Vector Store** using Pinecone for scalable semantic search
* ğŸŒ **Flask Web App** for interactive user interface
* ğŸ¨ Custom **CSS Styling**
* ğŸ§ª Organized trial notebooks for experimentation

---

## ğŸ—‚ï¸ Project Structure

```
LLMops-Project/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ medical.pdf                # Source medical PDF file
â”‚
â”œâ”€â”€ research/
â”‚   â””â”€â”€ trial.ipynb               # Jupyter notebook for testing and experimentation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prompt.py                 # Custom prompt templates
â”‚   â””â”€â”€ loader.py                 # PDF loading and text splitting logic
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css                 # UI Styling (Chat + Layout)
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                # Flask front-end HTML template
â”‚
â”œâ”€â”€ app.py                        # Main Flask application
â”œâ”€â”€ main.py                       # Core logic to initialize components
â”œâ”€â”€ vectore_store_db.py          # Pinecone vector store configuration
â”œâ”€â”€ templates.py                  # Templating and rendering logic
â”œâ”€â”€ setup.py                      # Environment and setup file
â””â”€â”€ requirements.txt              # Dependencies list
```

---

## âš™ï¸ Technologies Used

* **Python**
* **Flask** (Web framework)
* **LangChain** (LLM integration)
* **Pinecone** (Vector database)
* **OpenAI GPT / LLM model**
* **HTML/CSS** (Chat UI)
* **Jupyter Notebook** (for R\&D)

---

## ğŸ§ª How It Works

1. **PDF Ingestion**: The medical PDF is loaded, split into chunks, and converted into embeddings.
2. **Vector Storage**: These embeddings are stored in Pinecone for fast retrieval.
3. **User Query**: The chatbot accepts user queries from a Flask-powered frontend.
4. **Context Retrieval**: LangChain retrieves relevant chunks from Pinecone.
5. **LLM Response**: The context is passed to the LLM to generate an intelligent response.
6. **Display**: The response is shown in the browser along with timestamps.

---

## ğŸ“¦ Installation & Setup

```bash
# Clone the repository
git clone https://github.com/your-username/LLMops-Project.git
cd LLMops-Project

# Create environment (you are using 'uv' so we'll assume pip/conda environment)
pip install -r requirements.txt

# Start the Flask server
python app.py
```

> âš ï¸ Make sure you configure your **Pinecone API key** and **OpenAI API key** in a `.env` or config file before running.

---

## âœ… Requirements

* Python 3.9+
* `Flask`
* `langchain`
* `pinecone-client`
* `openai`
* `uvicorn` or local server setup
* `PyMuPDF` or `pdfplumber` for PDF reading (if used)

---

## ğŸ“¸ Screenshots

*You can add UI screenshots here.*

---

## ğŸ“Œ Future Improvements

* Add multi-PDF support
* Implement user authentication
* Chat history download/export
* Real-time LLM model switching (OpenAI, Cohere, etc.)

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to fork the repo and submit pull requests for improvements or new features.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

Would you like me to generate this as a `.md` file for direct upload to GitHub?
